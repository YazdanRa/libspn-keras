{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Sampling with conv SPNs.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw8l-b_NIaBy"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pronobis/libspn-keras/blob/master/examples/notebooks/Sampling%20with%20conv%20SPNs.ipynb)\n",
    "\n",
    "# **Image Sampling**: Sampling MNIST images\n",
    "In this notebook, we'll set up an SPN to generate new MNIST images by sampling from an SPN.\n",
    "\n",
    "First let's set up the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zpHo0-cLPIyD"
   },
   "source": [
    "!pip install libspn-keras matplotlib"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM7gCCcdfJ3U"
   },
   "source": [
    "## Convolutional SPN\n",
    "A convolutional SPN consists of convolutional product and convolutional sum nodes. For the sake of \n",
    "demonstration, we'll use a structure that trains relatively quickly, without worrying too much about the final performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "333_El0hJo8J"
   },
   "source": [
    "import libspn_keras as spnk"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0uKj7puJo8J"
   },
   "source": [
    "### Setting the Default Sum Accumulator Initializer\n",
    "\n",
    "In `libspn-keras`, we refer to the unnormalized weights as _accumulators_. These can be represented in linear space or logspace. Setting the ``SumOp`` also configures the default choice of representation space for these accumulators. For example, gradients should be used in the case of _discriminative_ learning and accumulators are then preferrably represented in logspace. This overcomes the need to project the accumulators to $\\mathbb R^+$ after gradient updates, since for log accumulators can take any value in $\\mathbb R$ (whereas linear accumulators are limited to $\\mathbb R^+$).\n",
    "\n",
    "In this case however, we'll do generative learning so we can set our `SumOp` to `SumOpEMBackprop`.\n",
    "\n",
    "To set the default initial value (which will be transformed to logspace internally if needed), one can use `spnk.set_default_accumulator_initializer`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GH_3p88eJo8K"
   },
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "spnk.set_default_accumulator_initializer(\n",
    "    spnk.initializers.Dirichlet()\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DWvCACQ50ROK"
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from libspn_keras.layers import NormalizeAxes\n",
    "import tensorflow as tf\n",
    "\n",
    "def take_first(a, b):\n",
    "  return tf.reshape(tf.cast(a, tf.float32), (-1, 28, 28, 1))\n",
    "\n",
    "normalize = spnk.layers.NormalizeStandardScore(\n",
    "    input_shape=(28, 28, 1), axes=NormalizeAxes.GLOBAL, \n",
    "    normalization_epsilon=1e-3\n",
    ")\n",
    "\n",
    "mnist_images = tfds.load(name=\"mnist\", batch_size=32, split=\"train\", as_supervised=True).map(take_first)\n",
    "normalize.adapt(mnist_images) \n",
    "mnist_normalized = mnist_images.map(normalize)\n",
    "location_initializer = spnk.initializers.PoonDomingosMeanOfQuantileSplit(\n",
    "    mnist_normalized\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZBUn_xYJo8K"
   },
   "source": [
    "### Defining the Architecture\n",
    "We'll go for a relatively simple convolutional SPN architecture. We use solely non-overlapping patches. After 5 convolutions, the nodes' scopes cover all variables. We then add a layer with 10 mixtures, one for each class. We can do this to optimize the joint probability of $P(X,Y)$ instead of just $P(X)$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3G4lXlqg0_0r"
   },
   "source": [
    "def build_spn(sum_op, return_logits, infer_no_evidence=False):\n",
    "  spnk.set_default_sum_op(sum_op)\n",
    "  return spnk.models.SequentialSumProductNetwork([\n",
    "    normalize,\n",
    "    spnk.layers.NormalLeaf(\n",
    "        num_components=4, \n",
    "        location_trainable=True,\n",
    "        location_initializer=location_initializer,\n",
    "        scale_trainable=True\n",
    "    ),\n",
    "    spnk.layers.Conv2DProduct(\n",
    "        depthwise=False, \n",
    "        strides=[2, 2], \n",
    "        dilations=[1, 1], \n",
    "        kernel_size=[2, 2],\n",
    "        padding='valid'\n",
    "    ),\n",
    "    spnk.layers.Local2DSum(num_sums=256),\n",
    "    spnk.layers.Conv2DProduct(\n",
    "        depthwise=True, \n",
    "        strides=[2, 2], \n",
    "        dilations=[1, 1], \n",
    "        kernel_size=[2, 2],\n",
    "        padding='valid'\n",
    "    ),\n",
    "    spnk.layers.Local2DSum(num_sums=512),\n",
    "    # Pad to go from 7x7 to 8x8, so that we can apply 3 more Conv2DProducts\n",
    "    tf.keras.layers.ZeroPadding2D(((0, 1), (0, 1))),\n",
    "    spnk.layers.Conv2DProduct(\n",
    "        depthwise=True, \n",
    "        strides=[2, 2], \n",
    "        dilations=[1, 1], \n",
    "        kernel_size=[2, 2],\n",
    "        padding='valid'\n",
    "    ),\n",
    "    spnk.layers.Local2DSum(num_sums=512),\n",
    "    spnk.layers.Conv2DProduct(\n",
    "        depthwise=True, \n",
    "        strides=[2, 2], \n",
    "        dilations=[1, 1], \n",
    "        kernel_size=[2, 2],\n",
    "        padding='valid'\n",
    "    ),\n",
    "    spnk.layers.Local2DSum(num_sums=1024),\n",
    "    spnk.layers.Conv2DProduct(\n",
    "        depthwise=True, \n",
    "        strides=[2, 2], \n",
    "        dilations=[1, 1], \n",
    "        kernel_size=[2, 2],\n",
    "        padding='valid'\n",
    "    ),\n",
    "    spnk.layers.LogDropout(rate=0.5),\n",
    "    spnk.layers.DenseSum(num_sums=10),\n",
    "    spnk.layers.RootSum(return_weighted_child_logits=return_logits)\n",
    "  ], infer_no_evidence=infer_no_evidence, unsupervised=False)\n",
    " "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "EEr_IcoLJo8K"
   },
   "source": [
    "sum_product_network = build_spn(spnk.SumOpEMBackprop(), return_logits=True)\n",
    "sum_product_network.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXN945AmcvNh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up a `tf.Dataset` with `tensorflow_datasets`\n",
    "Then, we'll configure a train set and a test set using `tensorflow_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wjmtjHUvct-q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "mnist_train = (\n",
    "    tfds.load(name=\"mnist\", split=\"train\", as_supervised=True)\n",
    "    .shuffle(1024)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "mnist_test = (\n",
    "    tfds.load(name=\"mnist\", split=\"test\", as_supervised=True)\n",
    "    .batch(100)\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLlgidYGJo8M"
   },
   "source": [
    "### Configuring the remaining training components\n",
    "Note that our SPN spits out the joint probabities for each $y\\in\\{Y_i\\}_{i=1}^{10}$, so there are 10 outputs per sample. We can optimize the probability of $P(X,Y)$ by using `spnk.metrics.NegativeLogJoint` as the loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8b20-VDbxbr7"
   },
   "source": [
    "optimizer = spnk.optimizers.OnlineExpectationMaximization(learning_rate=0.05, accumulate_batches=1)\n",
    "metrics = []\n",
    "loss = spnk.losses.NegativeLogJoint()\n",
    "\n",
    "sum_product_network.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3cQ2B8NdvK7"
   },
   "source": [
    "### Training the SPN\n",
    "We can simply use the `.fit` function that comes with Keras and pass our `tf.data.Dataset` to it to train!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ntIS2OEbdnku"
   },
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "sum_product_network.fit(mnist_train, epochs=20, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", min_delta=0.1, patience=2, factor=0.5)])\n",
    "sum_product_network.evaluate(mnist_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzfZGyELoB-S"
   },
   "source": [
    "## Building an SPN to sample\n",
    "For sampling, we require our sum nodes to backpropagate discrete signals that correspond to the sampled paths. Each\n",
    "path originates at the root and eventually ends up at the leaves. We can set the backprop op to\n",
    "`spnk.SumOpSampleBackprop` to ensure all sum layers propagate the discrete sample signal.\n",
    "\n",
    "We build using the same function as before and copy the weights from the already trained SPN."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xxda_6yF0tGP"
   },
   "source": [
    "sum_product_network_sample = build_spn(spnk.SumOpSampleBackprop(), return_logits=False, infer_no_evidence=True)\n",
    "sum_product_network_sample.set_weights(sum_product_network.get_weights())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRzu5q8apAuL"
   },
   "source": [
    "## Drawing samples\n",
    "Sampling from SPNs comes down to determining values for variables that are outside of the evidence. When images are\n",
    "sampled as a whole, all variables are omitted from the evidence. For this special case of inference,\n",
    "the `SequentialSumProductNetwork` class defines a `zero_evidence_inference` method that takes a size parameter.\n",
    "\n",
    "Below, we sample 64 images and voilá!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dHuwVIsD4WQX"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(12., 12.))\n",
    "grid = ImageGrid(\n",
    "    fig, 111,\n",
    "    nrows_ncols=(10, 10),\n",
    "    axes_pad=0.1,\n",
    ")\n",
    "\n",
    "sample = sum_product_network_sample.zero_evidence_inference(100)\n",
    "\n",
    "print(\"Sampling done... Now ploting results\")\n",
    "for ax, im in zip(grid, sample):\n",
    "    ax.imshow(np.squeeze(im), cmap=\"gray\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
