{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "libspn-keras DGC-SPN image classification tutorial.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw8l-b_NIaBy"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pronobis/libspn-keras/blob/master/examples/notebooks/DGC-SPN%20Image%20Classification.ipynb)\n",
    "\n",
    "# **Image Classification**: A Deep Generalized Convolutional Sum-Product Network (DGC-SPN) with `libspn-keras`\n",
    "Let's go through an example of building complex SPNs with [`libspn-keras`](https://github.com/pronobis/libspn-keras). The layer-based API of the library makes it straightforward to build advanced SPN architectures.\n",
    "\n",
    "First let's set up the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dwzmXEfUdFYR"
   },
   "source": [
    "!pip libspn-keras"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM7gCCcdfJ3U"
   },
   "source": [
    "## DGC-SPN\n",
    "A DGC-SPN consists of convolutional product and sum nodes. For the sake of \n",
    "demonstration, we'll use a structure that trains relatively quickly, without worrying too much about the final accuracy of the model. \n",
    "\n",
    "### Setting the Default Sum Operation\n",
    "\n",
    "We'll take the default approach to training deep models for classification through gradient-based minimization of\n",
    "cross-entropy. To ensure our sum operations in the SPN pass down gradients instead of EM signals, we set\n",
    "the default sum operation to `SumOpGradBackprop`. In fact, this is the default setting of `libspn-keras`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gTiUvFoMdFYS"
   },
   "source": [
    "import libspn_keras as spnk\n",
    "\n",
    "print(spnk.get_default_sum_op())\n",
    "\n",
    "spnk.set_default_sum_op(spnk.SumOpGradBackprop())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsMGRaNYdFYT"
   },
   "source": [
    "### Setting the Default Sum Accumulator Initializer\n",
    "\n",
    "In `libspn-keras`, we refer to the unnormalized weights as _accumulators_. These can be represented in linear space or logspace. Setting the ``SumOp`` also configures the default choice of representation space for these accumulators. For example, gradients should be used in the case of _discriminative_ learning and accumulators are then preferrably represented in logspace. This overcomes the need to project the accumulators to $\\mathbb R^+$ after gradient updates, since for log accumulators can take any value in $\\mathbb R$ (whereas linear accumulators are limited to $\\mathbb R^+$).\n",
    "\n",
    "To set the default initial value (which will be transformed to logspace internally if needed), one can use `spnk.set_default_accumulator_initializer`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yLHuQM7RdFYT"
   },
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "spnk.set_default_accumulator_initializer(\n",
    "    keras.initializers.TruncatedNormal(stddev=0.5, mean=1.0)\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFYyV7JvdFYT"
   },
   "source": [
    "### Defining the Architecture\n",
    "We begin by using non-overlapping convolution patches for our first two product layers,\n",
    "since this way we make sure that no scopes overlap between products. \n",
    "\n",
    "For the\n",
    "remainder of the product layers, we'll use *exponentially increasing dilation rates*. By doing so, we have\n",
    "'overlapping' patches that still yield a valid SPN. These exponentially increasing dilation rates for convolutional SPNs were first\n",
    "introduced in [this paper](https://arxiv.org/abs/1902.06155)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "inHBA-5peUnR"
   },
   "source": [
    "normalize = spnk.layers.NormalizeStandardScore(\n",
    "    axes=spnk.layers.NormalizeAxes.GLOBAL, input_shape=(28, 28, 1)) \n",
    "\n",
    "def take_first(a, b):\n",
    "  return a\n",
    "\n",
    "normalize.adapt(\n",
    "    tfds.load(name=\"mnist\", split=\"train\", as_supervised=True).map(take_first).batch(batch_size)\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kl4tigIWdFYU"
   },
   "source": [
    "sum_product_network = keras.Sequential([\n",
    "  normalize,\n",
    "  spnk.layers.NormalLeaf(\n",
    "      num_components=16, \n",
    "      location_trainable=True,\n",
    "      location_initializer=keras.initializers.TruncatedNormal(\n",
    "          stddev=1.0, mean=0.0)\n",
    "  ),\n",
    "  # Non-overlapping products\n",
    "  spnk.layers.Conv2DProduct(\n",
    "      depthwise=True, \n",
    "      strides=[2, 2], \n",
    "      dilations=[1, 1], \n",
    "      kernel_size=[2, 2],\n",
    "      padding='valid'\n",
    "  ),\n",
    "  spnk.layers.Local2DSum(num_sums=16),\n",
    "  # Non-overlapping products\n",
    "  spnk.layers.Conv2DProduct(\n",
    "      depthwise=True, \n",
    "      strides=[2, 2], \n",
    "      dilations=[1, 1], \n",
    "      kernel_size=[2, 2],\n",
    "      padding='valid'\n",
    "  ),\n",
    "  spnk.layers.Local2DSum(num_sums=32),\n",
    "  # Overlapping products, starting at dilations [1, 1]\n",
    "  spnk.layers.Conv2DProduct(\n",
    "      depthwise=True, \n",
    "      strides=[1, 1], \n",
    "      dilations=[1, 1], \n",
    "      kernel_size=[2, 2],\n",
    "      padding='full'\n",
    "  ),\n",
    "  spnk.layers.Local2DSum(num_sums=32),\n",
    "  # Overlapping products, with dilations [2, 2] and full padding\n",
    "  spnk.layers.Conv2DProduct(\n",
    "      depthwise=True, \n",
    "      strides=[1, 1], \n",
    "      dilations=[2, 2], \n",
    "      kernel_size=[2, 2],\n",
    "      padding='full'\n",
    "  ),\n",
    "  spnk.layers.Local2DSum(num_sums=64),\n",
    "  # Overlapping products, with dilations [2, 2] and full padding\n",
    "  spnk.layers.Conv2DProduct(\n",
    "      depthwise=True, \n",
    "      strides=[1, 1], \n",
    "      dilations=[4, 4], \n",
    "      kernel_size=[2, 2],\n",
    "      padding='full'\n",
    "  ),\n",
    "  spnk.layers.Local2DSum(num_sums=64),\n",
    "  # Overlapping products, with dilations [2, 2] and 'final' padding to combine \n",
    "  # all scopes\n",
    "  spnk.layers.Conv2DProduct(\n",
    "      depthwise=True, \n",
    "      strides=[1, 1], \n",
    "      dilations=[8, 8], \n",
    "      kernel_size=[2, 2],\n",
    "      padding='final'\n",
    "  ),\n",
    "  spnk.layers.SpatialToRegions(),\n",
    "  # Class roots\n",
    "  spnk.layers.DenseSum(num_sums=10),\n",
    "  spnk.layers.RootSum(return_weighted_child_logits=True)\n",
    "])\n",
    "\n",
    "sum_product_network.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXN945AmcvNh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up a `tf.Dataset` with `tensorflow_datasets`\n",
    "Then, we'll configure a train set and a test set using `tensorflow_datasets`. As first suggested in the work by [Poon and Domingos](https://arxiv.org/abs/1202.3732), we whiten each sample by subtracting the mean and dividing by the\n",
    "standard devation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wjmtjHUvct-q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "mnist_train = (\n",
    "    tfds.load(name=\"mnist\", split=\"train\", as_supervised=True)\n",
    "    .shuffle(1024)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "mnist_test = (\n",
    "    tfds.load(name=\"mnist\", split=\"test\", as_supervised=True)\n",
    "    .batch(100)\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrT_jenIdFYW"
   },
   "source": [
    "### Configuring the remaining training components\n",
    "Now that we have an SPN that produces logits for each class at it root, we can attach a `SparseCategoricalCrossEntropy` loss. Note that we need `from_logits=True` since the SPN computes its outputs in log-space!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8b20-VDbxbr7"
   },
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "sum_product_network.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3cQ2B8NdvK7"
   },
   "source": [
    "### Training the SPN\n",
    "We can simply use the `.fit` function that comes with Keras and pass our `tf.data.Dataset` to it to train!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ntIS2OEbdnku"
   },
   "source": [
    "sum_product_network.fit(mnist_train, epochs=15)\n",
    "sum_product_network.evaluate(mnist_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HH4yekrd-me"
   },
   "source": [
    "### Storing the SPN\n",
    "Finally, we might want to store our SPN, this is again a piece of cake using the `.save_weights` method of `tf.keras.models.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hg8eHcl5eStF"
   },
   "source": [
    "sum_product_network.save_weights('spn_weights.h5')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
